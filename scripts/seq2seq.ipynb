{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441e300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_filename = Path(\"../train_data/arithmetic__mixed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3ce262",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions to read data\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def ps(s):\n",
    "    \"\"\"Process String: convert a string into a list of lowercased words.\"\"\"\n",
    "    return [word.strip() for word in re.split(r'([+-/*()?]|\\d|\\w)', s) if word.strip()]\n",
    "\n",
    "def read_data(filepath,perc_data,max_q_len,max_a_len):\n",
    "    '''\n",
    "    returns character lists of questions and answers.\n",
    "    '''\n",
    "    # q,a lists\n",
    "    X = []\n",
    "    y = []\n",
    "    file_len = 0\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        # Grabbing a subset of the entire file\n",
    "        lines = f.readlines()\n",
    "        file_len = len(lines)\n",
    "        \n",
    "    with open(filepath) as dataset_file:\n",
    "        for i in range(0,int(file_len*perc_data)):\n",
    "            line_q = dataset_file.readline().strip()\n",
    "            line_a = dataset_file.readline().strip()\n",
    "            if(len(line_q) < max_q_len and len(line_a) < max_a_len):\n",
    "                X.append(ps(line_q))\n",
    "                y.append(ps(line_a))   \n",
    "    return X,y\n",
    "\n",
    "def pad_data(X,y, max_question_len, max_answer_len):\n",
    "    # dataset is of form [(q,a)]\n",
    "    X_padded = list()\n",
    "    for q in X:\n",
    "        qpad =  ['BOE'] + q + ['EOE'] + ['#' for _ in range(max_question_len-len(q))] \n",
    "        X_padded.append(qpad)\n",
    "    y_padded = list()\n",
    "    for a in y:\n",
    "        apad =  ['BOE']  + a + ['EOE'] + ['#' for _ in range(max_answer_len-len(a))]\n",
    "        y_padded.append(apad)\n",
    "    return X_padded,y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c044e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alphabet_index(X):\n",
    "\n",
    "    char_to_int = {}\n",
    "    char_to_int['#'] = 0\n",
    "    for q in X:\n",
    "        for word in q:\n",
    "            if word not in char_to_int:\n",
    "                char_to_int[word] = len(char_to_int)\n",
    "    \n",
    "    int_to_char = dict([(char_to_int[char],char) for char in char_to_int])\n",
    "\n",
    "    return (char_to_int,int_to_char)\n",
    "\n",
    "def encode_data(X,y,char_to_int):\n",
    "    Xenc = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    \n",
    "    return Xenc, yenc\n",
    "\n",
    "# one hot encode\n",
    "def one_hot_encode(X, y, vocab_size):\n",
    "\tXenc = list()\n",
    "\tfor seq in X:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(vocab_size)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tXenc.append(pattern)\n",
    "\tyenc = list()\n",
    "\tfor seq in y:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(vocab_size)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tyenc.append(pattern)\n",
    "\treturn Xenc, yenc\n",
    "\n",
    "def one_hot_decode(seq, int_to_char):\n",
    "\tstrings = list()\n",
    "\tfor pattern in seq:\n",
    "\t\tstring = int_to_char[np.argmax(pattern)]\n",
    "\t\tstrings.append(string)\n",
    "\treturn ''.join(strings)\n",
    "\n",
    "def process_data(dataset_filename, perc_data,max_q_len,max_a_len):\n",
    "    X,y = read_data(dataset_filename,perc_data,max_q_len,max_a_len)\n",
    "    X,y = pad_data(X,y,max_q_len,max_a_len)\n",
    "    X_train,X_test,y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "    char_to_int,int_to_char = create_alphabet_index(X_train)\n",
    "    X_train,y_train = encode_data(X_train,y_train,char_to_int)\n",
    "    X_test,y_test = encode_data(X_test,y_test,char_to_int)\n",
    "    # X,y = one_hot_encode(X,y,len(char_to_int))\n",
    "    return (X_train, y_train, X_test,y_test,char_to_int,int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_experiment(trial,i):\n",
    "    \n",
    "    n_batch =  trial[\"n_batch\"]\n",
    "    n_epoch =  trial[\"n_epoch\"]\n",
    "    data_perc = trial[\"data_perc\"]\n",
    "    embed_dim = trial[\"embed_dim\"]\n",
    "    encoder_hid = trial[\"encoder_hid\"]\n",
    "    decoder_hid = trial[\"decoder_hid\"]\n",
    "    max_q_len = trial[\"max_q_len\"]\n",
    "    max_a_len = trial[\"max_a_len\"]\n",
    "    \n",
    "    X_train,y_train,X_test,y_test, char_to_int,int_to_char = process_data(dataset_filename, data_perc,max_q_len,max_a_len)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(char_to_int), embed_dim, input_length=max_q_len+2, mask_zero = True))\n",
    "    model.add(LSTM(encoder_hid, input_shape=(max_q_len+2, len(char_to_int))))\n",
    "    model.add(RepeatVector(max_a_len+2))\n",
    "    model.add(LSTM(decoder_hid, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(len(char_to_int), activation='softmax'))),\n",
    "    checkpoint_filepath = Path(\"../tmp/checkpoint/{:d}\".format(i))\n",
    "    \n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    _, y_train = one_hot_encode([],y_train, len(char_to_int))\n",
    "    _, y_test = one_hot_encode([],y_test,len(char_to_int))\n",
    "    # train LSTM\n",
    "    history = model.fit(np.array(X_train), np.array(y_train), validation_data=(X_test, y_test),\n",
    "                        epochs=n_epoch, batch_size=n_batch, callbacks=[early_stop_callback,model_checkpoint_callback])\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"figs/acc/trial_{:d}_acc.png\".format(i))\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"figs/loss/trial_{:d}_loss.png\".format(i))\n",
    "    plt.show()\n",
    "    js = {\n",
    "        \"parameters\": trial,\n",
    "         \"history\": history.history\n",
    "    }\n",
    "\n",
    "    with open(\"json/trial_{:d}.json\".format(i), \"w\") as outfile:\n",
    "        json.dump(js, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110a7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trials():\n",
    "    trials = []\n",
    "    data_percs = [0.1,0.5]\n",
    "    n_batchs = [128]\n",
    "    n_epochs =  [100]\n",
    "    embed_dims = [30,100]\n",
    "    encoder_hids = [128,256]\n",
    "    decoder_hids = [128,256]\n",
    "    max_lens = [(20,5),(30,10)]\n",
    "    for d_h in decoder_hids:\n",
    "        for n_b in n_batchs:   \n",
    "            for e_h in encoder_hids: \n",
    "                for e_d in embed_dims: \n",
    "                    for n_e in n_epochs:\n",
    "                        for m_l in max_lens:\n",
    "                            for p in data_percs:\n",
    "                                trials.append(\n",
    "                                    {\n",
    "                                        \"n_batch\" :  n_b,\n",
    "                                        \"n_epoch\" :  n_e,\n",
    "                                        \"data_perc\" : p,\n",
    "                                        \"embed_dim\" : e_d,\n",
    "                                        \"encoder_hid\" : e_h,\n",
    "                                        \"decoder_hid\" : d_h,\n",
    "                                        \"max_q_len\" : m_l[0],\n",
    "                                        \"max_a_len\" : m_l[1]   \n",
    "                                    })\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07327b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 17:58:01.320045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-29 17:58:01.320071: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-29 17:58:01.320302: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 22, 30)            1020      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               81408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 7, 128)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 7, 128)            131584    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 7, 34)            4386      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,398\n",
      "Trainable params: 218,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "47/73 [==================>...........] - ETA: 1s - loss: 2.2710 - accuracy: 0.4233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1109684/1922128672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# be careful not to overwrite existing files. Change i to i + 10 maybe if you already did 10 trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtry_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1109684/4233573008.py\u001b[0m in \u001b[0;36mtry_experiment\u001b[0;34m(trial, i)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# train LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     history = model.fit(np.array(X_train), np.array(y_train), validation_data=(X_test, y_test),\n\u001b[0m\u001b[1;32m     37\u001b[0m                         epochs=n_epoch, batch_size=n_batch, callbacks=[early_stop_callback,model_checkpoint_callback])\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = []\n",
    "trials.append({\"n_batch\" :  128, \"n_epoch\" :  100, \"data_perc\" : 0.1,\"embed_dim\" : 30, \n",
    "               \"encoder_hid\" : 256,\"decoder_hid\" : 128, \"max_q_len\" : 20, \"max_a_len\" : 5},\n",
    "             {\"n_batch\" :  128, \"n_epoch\" :  100, \"data_perc\" : 0.5,\"embed_dim\" : 30, \n",
    "               \"encoder_hid\" : 256,\"decoder_hid\" : 128, \"max_q_len\" : 20, \"max_a_len\" : 5},\n",
    "             {\"n_batch\" :  128, \"n_epoch\" :  100, \"data_perc\" : 0.1,\"embed_dim\" : 30, \n",
    "               \"encoder_hid\" : 256,\"decoder_hid\" : 128, \"max_q_len\" : 30, \"max_a_len\" : 10},\n",
    "             {\"n_batch\" :  128, \"n_epoch\" :  100, \"data_perc\" : 0.5,\"embed_dim\" : 30, \n",
    "               \"encoder_hid\" : 256,\"decoder_hid\" : 128, \"max_q_len\" : 30, \"max_a_len\" : 10})\n",
    "# or alternatively, create this manually: \n",
    "# trials.append({\"n_batch\" :  n_b, \"n_epoch\" :  n_e, \"data_perc\" : p,\"embed_dim\" : e_d, \n",
    "#               \"encoder_hid\" : e_h,\"decoder_hid\" : d_h, \"max_q_len\" : m_q_len, \"max_a_len\" : m_q_len})\n",
    "# be careful not to overwrite existing files. Change i to i + 10 maybe if you already did 10 trials\n",
    "for i,trial in enumerate(trials):\n",
    "    try_experiment(trial,i+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on some new patterns\n",
    "result = model.predict(X_test[:100], verbose=0)\n",
    "# calculate error\n",
    "expected = [one_hot_decode(x, int_to_char) for x in y_test[:100]]\n",
    "predicted = [one_hot_decode(x, int_to_char) for x in result]\n",
    "# show some examples\n",
    "for i in range(100):\n",
    "\tprint('Expected=%s, Predicted=%s' % (expected[i], predicted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87aed1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method to generate dataset, in case our original dataset doesn't work.\n",
    "\n",
    "valid_characters = '0123456789.+*-/ '\n",
    "char_to_int = dict((character, index) for index, character in  enumerate(valid_characters))\n",
    "int_to_char = dict((index, character) for index, character in  enumerate(valid_characters))\n",
    "\n",
    "number_max = 100 #Up to this number\n",
    "MAX_Q_LEN = len(str(number_max-1)) * 2 + 1\n",
    "MAX_A_LEN = MAX_Q_LEN\n",
    "operators = ['+', '*', '-', '/']\n",
    "operators_dict = { \"+\":operator.add, \n",
    "                  '*':operator.mul, \n",
    "                  \"-\":operator.sub,\n",
    "                  '/':operator.truediv}\n",
    "\n",
    "def oper_generator():\n",
    "    number_1 = np.random.randint(1,number_max)\n",
    "    operator_index = np.random.randint(0,len(operators))\n",
    "    operator = operators[operator_index]\n",
    "    number_2 = np.random.randint(1,number_max)\n",
    "    number_1= max(number_1,number_2)\n",
    "    number_2= min(number_1,number_2)\n",
    "    operation = str(number_1) + operator + str(number_2)\n",
    "    result = str(operators_dict[operator](number_1,number_2))[:MAX_A_LEN]\n",
    "    return ps(operation), ps(result)\n",
    "\n",
    "def data_generator(training_size,test_size):\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in (range(0, training_size)):\n",
    "        x, y = oper_generator()\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "    for i in (range(0, test_size)):\n",
    "        x, y = oper_generator()\n",
    "        x_test.append(x)\n",
    "        y_test.append(y)\n",
    "    X,y = pad_data(x_train,y_train,MAX_Q_LEN,MAX_A_LEN)\n",
    "    X,y = encode_data(X,y,char_to_int)\n",
    "    X,y = one_hot_encode(X,y,len(char_to_int))\n",
    "    X_test,y_test = pad_data(x_test,y_test,MAX_Q_LEN,MAX_A_LEN)\n",
    "    X_test,y_test = encode_data(X_test,y_test,char_to_int)\n",
    "    X_test,y_test = one_hot_encode(X_test,y_test,len(char_to_int))\n",
    "    return X,y,X_test,y_test\n",
    "\n",
    "data_points = 10000\n",
    "test_size = 0.2\n",
    "training_size = int(round(data_points * (1-test_size),0))\n",
    "test_size = data_points - training_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
