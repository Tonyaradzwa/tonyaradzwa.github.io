{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441e300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_filename = Path(\"../train_data/arithmetic__mixed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3ce262",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions to read data\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def ps(s):\n",
    "    \"\"\"Process String: convert a string into a list of lowercased words.\"\"\"\n",
    "    return [word.strip() for word in re.split(r'([+-/*()?]|\\d|\\w)', s) if word.strip()]\n",
    "\n",
    "def read_data(filepath,perc_data,max_q_len,max_a_len):\n",
    "    '''\n",
    "    returns character lists of questions and answers.\n",
    "    '''\n",
    "    # q,a lists\n",
    "    X = []\n",
    "    y = []\n",
    "    file_len = 0\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        # Grabbing a subset of the entire file\n",
    "        lines = f.readlines()\n",
    "        file_len = len(lines)\n",
    "        \n",
    "    with open(filepath) as dataset_file:\n",
    "        for i in range(0,int(file_len*perc_data)):\n",
    "            line_q = dataset_file.readline().strip()\n",
    "            line_a = dataset_file.readline().strip()\n",
    "            if(len(line_q) < max_q_len and len(line_a) < max_a_len):\n",
    "                X.append(ps(line_q))\n",
    "                y.append(ps(line_a))   \n",
    "    return X,y\n",
    "\n",
    "def pad_data(X,y, max_question_len, max_answer_len):\n",
    "    # dataset is of form [(q,a)]\n",
    "    X_padded = list()\n",
    "    for q in X:\n",
    "        qpad =  ['BOE'] + q + ['EOE'] + ['#' for _ in range(max_question_len-len(q))] \n",
    "        X_padded.append(qpad)\n",
    "    y_padded = list()\n",
    "    for a in y:\n",
    "        apad =  ['BOE']  + a + ['EOE'] + ['#' for _ in range(max_answer_len-len(a))]\n",
    "        y_padded.append(apad)\n",
    "    return X_padded,y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c044e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alphabet_index(X):\n",
    "\n",
    "    char_to_int = {}\n",
    "    char_to_int['#'] = 0\n",
    "    for q in X:\n",
    "        for word in q:\n",
    "            if word not in char_to_int:\n",
    "                char_to_int[word] = len(char_to_int)\n",
    "    \n",
    "    int_to_char = dict([(char_to_int[char],char) for char in char_to_int])\n",
    "\n",
    "    return (char_to_int,int_to_char)\n",
    "\n",
    "def encode_data(X,y,char_to_int):\n",
    "    Xenc = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    \n",
    "    return Xenc, yenc\n",
    "\n",
    "# one hot encode\n",
    "def one_hot_encode(X, y, vocab_size):\n",
    "\tXenc = list()\n",
    "\tfor seq in X:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(vocab_size)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tXenc.append(pattern)\n",
    "\tyenc = list()\n",
    "\tfor seq in y:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(vocab_size)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tyenc.append(pattern)\n",
    "\treturn Xenc, yenc\n",
    "\n",
    "def one_hot_decode(seq, int_to_char):\n",
    "\tstrings = list()\n",
    "\tfor pattern in seq:\n",
    "\t\tstring = int_to_char[np.argmax(pattern)]\n",
    "\t\tstrings.append(string)\n",
    "\treturn ''.join(strings)\n",
    "\n",
    "def process_data(dataset_filename, perc_data,max_q_len,max_a_len):\n",
    "    X,y = read_data(dataset_filename,perc_data,max_q_len,max_a_len)\n",
    "    X,y = pad_data(X,y,max_q_len,max_a_len)\n",
    "    X_train,X_test,y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "    char_to_int,int_to_char = create_alphabet_index(X_train)\n",
    "    X_train,y_train = encode_data(X_train,y_train,char_to_int)\n",
    "    X_test,y_test = encode_data(X_test,y_test,char_to_int)\n",
    "    # X,y = one_hot_encode(X,y,len(char_to_int))\n",
    "    return (X_train, y_train, X_test,y_test,char_to_int,int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2464bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def remove_padding(questions, expec, pred,i):\n",
    "    lines = []\n",
    "    lines.append(\"Question,Expected,Predicted\")\n",
    "    exp = re.compile(\"BOE(.*)EOE#*\")\n",
    "    count = 0\n",
    "    corr = 0\n",
    "    inc_for = 0\n",
    "    for i,line in enumerate(questions):\n",
    "        count += 1\n",
    "        if(re.findall(exp, pred[i])):\n",
    "            lines.append(\"{:s},{:s},{:s}\".format(re.findall(exp, line)[0],re.findall(exp, expec[i])[0], re.findall(exp, pred[i])[0]))\n",
    "            corr += (re.findall(exp, expec[i])[0] == re.findall(exp, pred[i])[0])\n",
    "        else:\n",
    "            inc_for +=1\n",
    "            \n",
    "    print(\"acc.\", corr/count)\n",
    "    print(\"inc. format\", inc_for/count)\n",
    "    f = open('../val_results/{:d}.csv'.format(i), 'w')\n",
    "    w = csv.writer(f, delimiter = ',')\n",
    "    w.writerows([x.split(',') for x in lines])\n",
    "    f.close()\n",
    "\n",
    "def dec_index(X_test, int_to_char):\n",
    "    qs = []\n",
    "    for x in X_test:\n",
    "        qs.append(''.join([int_to_char[a] for a in x]))\n",
    "    return qs\n",
    "\n",
    "#evaluate_test(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f3f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history,i):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"figs/acc/trial_{:d}_acc.png\".format(i))\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"figs/loss/trial_{:d}_loss.png\".format(i))\n",
    "    plt.show()\n",
    "    \n",
    "def dump_experiment_json(trial,history,i):\n",
    "    js = {\n",
    "            \"parameters\": trial,\n",
    "             \"history\": history.history\n",
    "      }\n",
    "    with open(\"json/trial_{:d}.json\".format(i), \"w\") as outfile:\n",
    "        json.dump(js, outfile)\n",
    "\n",
    "def evaluate_test(X_test,y_test,i,int_to_char):\n",
    "    new_model = tf.keras.models.load_model('model_{:d}'.format(i))\n",
    "    result = new_model.predict(X_test, verbose=0)\n",
    "    # calculate error\n",
    "    expected = [one_hot_decode(x, int_to_char) for x in y_test]\n",
    "    predicted = [one_hot_decode(x, int_to_char) for x in result]\n",
    "        # show some examples\n",
    "    qs = dec_index(X_test,int_to_char)\n",
    "    remove_padding(qs, expected, predicted,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "882677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_experiment(trial,i):\n",
    "    \n",
    "    n_batch =  trial[\"n_batch\"]\n",
    "    n_epoch =  trial[\"n_epoch\"]\n",
    "    data_perc = trial[\"data_perc\"]\n",
    "    embed_dim = trial[\"embed_dim\"]\n",
    "    encoder_hid = trial[\"encoder_hid\"]\n",
    "    decoder_hid = trial[\"decoder_hid\"]\n",
    "    max_q_len = trial[\"max_q_len\"]\n",
    "    max_a_len = trial[\"max_a_len\"]\n",
    "    \n",
    "    X_train,y_train,X_test,y_test, char_to_int,int_to_char = process_data(dataset_filename, data_perc,max_q_len,max_a_len)\n",
    "    print(\"size X_train:\", len(X_train), \"size X_test:\", len(X_test))\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(char_to_int), embed_dim, input_length=max_q_len+2, mask_zero = True))\n",
    "    model.add(LSTM(encoder_hid, input_shape=(max_q_len+2, len(char_to_int))))\n",
    "    model.add(RepeatVector(max_a_len+2))\n",
    "    model.add(LSTM(decoder_hid, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(len(char_to_int), activation='softmax'))),\n",
    "    checkpoint_filepath = Path(\"../tmp/checkpoint/{:d}.ckpt\".format(i))\n",
    "    \n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    _, y_train = one_hot_encode([],y_train, len(char_to_int))\n",
    "    _, y_test = one_hot_encode([],y_test,len(char_to_int))\n",
    "    # train LSTM\n",
    "    history = model.fit(np.array(X_train), np.array(y_train), validation_data=(X_test, y_test),\n",
    "                        epochs=n_epoch, batch_size=n_batch, callbacks=[early_stop_callback])\n",
    "    \n",
    "    plot_results(history,i)\n",
    "    dump_experiment_json(trial,history,i)\n",
    "    model.save('model_{:d}'.format(i))\n",
    "    evaluate_test(X_test,y_test,i,int_to_char)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07327b02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 32, 30)            1080      \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 128)               81408     \n",
      "                                                                 \n",
      " repeat_vector_5 (RepeatVect  (None, 12, 128)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 12, 128)           131584    \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 12, 36)           4644      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,716\n",
      "Trainable params: 218,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trials = [{\"n_batch\": 128, \"n_epoch\": 100, \"data_perc\": 0.1, \"embed_dim\": 30,\n",
    "           \"encoder_hid\": 128, \"decoder_hid\": 128, \"max_q_len\": 30, \"max_a_len\": 10}]\n",
    "# or alternatively, create this manually: \n",
    "# trials.append({\"n_batch\" :  n_b, \"n_epoch\" :  n_e, \"data_perc\" : p,\"embed_dim\" : e_d, \n",
    "#               \"encoder_hid\" : e_h,\"decoder_hid\" : d_h, \"max_q_len\" : m_q_len, \"max_a_len\" : m_q_len})\n",
    "# be careful not to overwrite existing files. Change i to i + 10 maybe if you already did 10 trials\n",
    "h=7\n",
    "for i,trial in enumerate(trials):\n",
    "    try_experiment(trial,i+h)\n",
    "\n",
    "#acc. 0.66994276369583\n",
    "#inc. format 0.006312346688470973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110a7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trials():\n",
    "    trials = []\n",
    "    data_percs = [0.1,0.5]\n",
    "    n_batchs = [128]\n",
    "    n_epochs =  [100]\n",
    "    embed_dims = [30,100]\n",
    "    encoder_hids = [128,256]\n",
    "    decoder_hids = [128,256]\n",
    "    max_lens = [(20,5),(30,10)]\n",
    "    for d_h in decoder_hids:\n",
    "        for n_b in n_batchs:   \n",
    "            for e_h in encoder_hids: \n",
    "                for e_d in embed_dims: \n",
    "                    for n_e in n_epochs:\n",
    "                        for m_l in max_lens:\n",
    "                            for p in data_percs:\n",
    "                                trials.append(\n",
    "                                    {\n",
    "                                        \"n_batch\" :  n_b,\n",
    "                                        \"n_epoch\" :  n_e,\n",
    "                                        \"data_perc\" : p,\n",
    "                                        \"embed_dim\" : e_d,\n",
    "                                        \"encoder_hid\" : e_h,\n",
    "                                        \"decoder_hid\" : d_h,\n",
    "                                        \"max_q_len\" : m_l[0],\n",
    "                                        \"max_a_len\" : m_l[1]   \n",
    "                                    })\n",
    "    return trials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
