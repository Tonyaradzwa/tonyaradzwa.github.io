{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd53252",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparing-data-for-use-as-NN-input\" data-toc-modified-id=\"Preparing-data-for-use-as-NN-input-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparing data for use as NN input</a></span></li><li><span><a href=\"#Letting-the-NN-parameterize-words\" data-toc-modified-id=\"Letting-the-NN-parameterize-words-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Letting the NN parameterize words</a></span></li><li><span><a href=\"#Adding-an-LSTM-layer\" data-toc-modified-id=\"Adding-an-LSTM-layer-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Adding an LSTM layer</a></span></li><li><span><a href=\"#Classifiying-the-LSTM-output\" data-toc-modified-id=\"Classifiying-the-LSTM-output-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Classifiying the LSTM output</a></span></li><li><span><a href=\"#Creating-training-and-validation-datasets\" data-toc-modified-id=\"Creating-training-and-validation-datasets-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Creating training and validation datasets</a></span></li><li><span><a href=\"#Creating-the-Parts-of-Speech-LSTM-model\" data-toc-modified-id=\"Creating-the-Parts-of-Speech-LSTM-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Creating the Parts of Speech LSTM model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Examining-results\" data-toc-modified-id=\"Examining-results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Examining results</a></span></li><li><span><a href=\"#Using-the-model-for-inference\" data-toc-modified-id=\"Using-the-model-for-inference-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Using the model for inference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08c513",
   "metadata": {},
   "source": [
    "# Predicting parts of speech with an LSTM\n",
    "\n",
    "Let's preview the end result. We want to take a sentence and output the part of speech for each word in that sentence. Something like this:\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "new_sentence = \"I is a teeth\"\n",
    "\n",
    "...\n",
    "\n",
    "predictions = model(processed_sentence)\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```text\n",
    "I     => Noun\n",
    "is    => Verb\n",
    "a     => Determiner\n",
    "teeth => Noun\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b2ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps(s):\n",
    "    \"\"\"Process String: convert a string into a list of lowercased words.\"\"\"\n",
    "    line = s.strip().replace(\" \", \"\")\n",
    "    return [c for c in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b2051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# read quesitons and answers from file\n",
    "dataset_filename = Path(\"../train_data/arithmetic__mixed.txt\")\n",
    "\n",
    "# questions = [ [\"1\", \"+\" , \"3\"], ... ]\n",
    "questions = []\n",
    "\n",
    "# answers = [ [] ]\n",
    "answers = []\n",
    "\n",
    "with open(dataset_filename) as dataset_file:\n",
    "    # Grabbing a subset of the entire file\n",
    "    for i in range(100):\n",
    "        line_q = dataset_file.readline().strip()\n",
    "        line_a = dataset_file.readline().strip()\n",
    "\n",
    "        questions.append([word.strip() for word in re.split(r'([+-/*()]|\\s+)', line_q) if word.strip()])\n",
    "        answers.append(eval(line_a))\n",
    "\n",
    "# use zip to create dataset object\n",
    "dataset = [(q,a) for q,a in zip(questions,answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699caa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855326b",
   "metadata": {},
   "source": [
    "## Preparing data for use as NN input\n",
    "\n",
    "We can't pass a list of plain text words and tags to a NN. We need to convert them to a more appropriate format.\n",
    "\n",
    "We'll start by creating a unique index for each word and tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e114fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "total_words = 0\n",
    "\n",
    "for question, _ in dataset:\n",
    "\n",
    "    total_words += len(question)\n",
    "\n",
    "    for word in question:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a91d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Vocabulary Indices\n",
      "-------------------------------\n",
      "             ( =>  2\n",
      "             ) =>  6\n",
      "             * => 25\n",
      "             + =>  1\n",
      "             - =>  4\n",
      "             . => 18\n",
      "             / =>  7\n",
      "             0 => 48\n",
      "             1 =>  8\n",
      "            10 =>  9\n",
      "           100 => 46\n",
      "          1008 => 95\n",
      "          1017 => 130\n",
      "           102 => 96\n",
      "           104 => 68\n",
      "           105 => 75\n",
      "          108? => 136\n",
      "            11 => 83\n",
      "           114 => 56\n",
      "           118 => 121\n",
      "            12 => 14\n",
      "           120 => 116\n",
      "           122 => 60\n",
      "           126 => 135\n",
      "          1271 => 27\n",
      "            13 => 90\n",
      "           130 => 118\n",
      "           133 => 82\n",
      "           135 => 142\n",
      "          1368 => 109\n",
      "            14 => 11\n",
      "           140 => 98\n",
      "           142 => 58\n",
      "           145 => 139\n",
      "            15 =>  0\n",
      "            16 => 20\n",
      "           160 => 79\n",
      "         16095 => 141\n",
      "           164 => 89\n",
      "           168 => 132\n",
      "            17 =>  5\n",
      "           174 => 23\n",
      "            18 => 40\n",
      "           180 => 43\n",
      "           182 => 69\n",
      "           188 => 119\n",
      "           18? => 133\n",
      "            19 => 72\n",
      "           198 => 15\n",
      "             2 => 29\n",
      "            20 => 33\n",
      "           200 => 117\n",
      "          2016 => 105\n",
      "           203 => 24\n",
      "            21 => 61\n",
      "           210 => 104\n",
      "          225? => 143\n",
      "           228 => 92\n",
      "          2296 => 28\n",
      "            24 => 12\n",
      "            25 => 34\n",
      "            26 => 52\n",
      "          2625 => 74\n",
      "            27 => 39\n",
      "            28 => 115\n",
      "           287 => 100\n",
      "           28? => 137\n",
      "             3 => 26\n",
      "            30 => 38\n",
      "            31 => 65\n",
      "           315 => 44\n",
      "          3198 => 91\n",
      "            32 => 120\n",
      "           320 => 77\n",
      "           329 => 127\n",
      "            33 => 111\n",
      "          3325 => 63\n",
      "            34 => 36\n",
      "           341 => 85\n",
      "            35 => 80\n",
      "            36 => 101\n",
      "            39 => 45\n",
      "            3? => 110\n",
      "             4 => 22\n",
      "            40 => 122\n",
      "           400 => 73\n",
      "           40? => 62\n",
      "            42 => 53\n",
      "           432 => 66\n",
      "            44 => 86\n",
      "           444 => 140\n",
      "            45 => 97\n",
      "           460 => 107\n",
      "            48 => 78\n",
      "           492 => 99\n",
      "           495 => 87\n",
      "           497 => 59\n",
      "            4? => 84\n",
      "             5 => 21\n",
      "            50 => 131\n",
      "            51 => 113\n",
      "           51? => 54\n",
      "            54 => 67\n",
      "            56 => 47\n",
      "           560 => 76\n",
      "           565 => 129\n",
      "            57 => 64\n",
      "            58 => 88\n",
      "             6 => 37\n",
      "            60 => 10\n",
      "           600 => 16\n",
      "            65 => 70\n",
      "            66 => 106\n",
      "           665 => 108\n",
      "           675 => 102\n",
      "            6? => 134\n",
      "             7 =>  3\n",
      "            70 => 103\n",
      "          7011 => 93\n",
      "           738 => 123\n",
      "            75 => 128\n",
      "            76 => 57\n",
      "            78 => 41\n",
      "           78? => 112\n",
      "            7? => 114\n",
      "             8 => 55\n",
      "            82 => 124\n",
      "            83 => 125\n",
      "            84 => 71\n",
      "           861 => 94\n",
      "            87 => 138\n",
      "             9 => 30\n",
      "            92 => 42\n",
      "            98 => 81\n",
      "            99 => 17\n",
      "            9? => 126\n",
      "             ? => 35\n",
      "     Calculate => 13\n",
      "      Evaluate => 19\n",
      "          What => 31\n",
      "            is => 32\n",
      "            of => 51\n",
      "           the => 49\n",
      "         value => 50\n",
      "\n",
      "Total number of words: 1758\n",
      "Number of unique words: 144\n"
     ]
    }
   ],
   "source": [
    "print(\"       Vocabulary Indices\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "for word in sorted(word_to_index):\n",
    "    print(f\"{word:>14} => {word_to_index[word]:>2}\")\n",
    "\n",
    "print(\"\\nTotal number of words:\", total_words)\n",
    "print(\"Number of unique words:\", len(word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833357a4",
   "metadata": {},
   "source": [
    "## Letting the NN parameterize words\n",
    "\n",
    "Once we have a unique identifier for each word, it is useful to start our NN with an [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) layer. This layer converts an index into a vector of values.\n",
    "\n",
    "You can think of each value as indicating something about the word. For example, maybe the first value indicates how much a word conveys happiness vs sadness. Of course, the NN can learn any attributes and it is not limited to thinks like happy/sad, masculine/feminine, etc.\n",
    "\n",
    "**Creating an embedding layer**. An embedding layer is created by telling it the size of the vocabulary (the number of words) and an embedding dimension (how many values to use to represent a word).\n",
    "\n",
    "**Embedding layer input and output**. An embedding layer takes an index and return a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a695b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index_tensor(words, mapping):\n",
    "    indices = [mapping[w] for w in words]\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20c8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embed_dim = 6  # Hyperparameter\n",
    "embed_layer = torch.nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24739098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13]),\n",
       " torch.Size([13, 6]),\n",
       " tensor([[-1.0585,  0.0547,  2.2342, -0.7027,  0.3794,  0.3668],\n",
       "         [-1.6456,  0.4101, -0.9782, -0.1616, -2.1837, -0.7466],\n",
       "         [ 0.4347, -1.2462,  0.2102,  0.0891,  0.1622, -0.5003],\n",
       "         [-0.1166,  0.9855,  0.1602,  1.2682, -0.8762,  2.1339],\n",
       "         [-1.7625,  1.9052, -1.6493, -0.4016,  0.7758,  0.0818],\n",
       "         [ 0.4347, -1.2462,  0.2102,  0.0891,  0.1622, -0.5003],\n",
       "         [ 0.1691, -0.6900, -0.1043, -0.4392,  0.8320, -0.0137],\n",
       "         [-1.0585,  0.0547,  2.2342, -0.7027,  0.3794,  0.3668],\n",
       "         [-1.7625,  1.9052, -1.6493, -0.4016,  0.7758,  0.0818],\n",
       "         [ 1.4024, -0.2317, -0.8802,  2.1191,  0.1747,  1.4870],\n",
       "         [ 0.1328,  0.1640, -1.2090, -0.4015, -2.1778,  0.9543],\n",
       "         [-1.0585,  0.0547,  2.2342, -0.7027,  0.3794,  0.3668],\n",
       "         [-0.5316, -0.5064,  0.4487,  0.1517,  0.6270,  1.6644]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = torch.tensor([word_to_index[\"the\"], word_to_index[\"dog\"]])\n",
    "indices = convert_to_index_tensor(ps(\"15 + (7 + -17)/12\"), word_to_index)\n",
    "embed_output = embed_layer(indices)\n",
    "indices.shape, embed_output.shape, embed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e167a05",
   "metadata": {},
   "source": [
    "## Adding an LSTM layer\n",
    "\n",
    "The [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) layer is in charge of processing embeddings such that the network can output the correct classification. Since this is a recurrent layer, it will take into account past words when it creates an output for the current word.\n",
    "\n",
    "**Creating an LSTM layer**. To create an LSTM you need to tell it the size of its input (the size of an embedding) and the size of its internal cell state.\n",
    "\n",
    "**LSTM layer input and output**. An LSTM takes an embedding (and optionally an initial hidden and cell state) and outputs a value for each word as well as the current hidden and cell state).\n",
    "\n",
    "If you read the linked LSTM documentation you will see that it requires input in this format: (seq_len, batch, input_size)\n",
    "\n",
    "As you can see above, our embedding layer outputs something that is (seq_len, input_size). So, we need to add a dimension in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4284a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10  # Hyperparameter\n",
    "num_layers = 5  # Hyperparameter\n",
    "lstm_layer = torch.nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486bb0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The LSTM layer expects the input to be in the shape (L, N, E)\n",
    "#   L is the length of the sequence\n",
    "#   N is the batch size (we'll stick with 1 here)\n",
    "#   E is the size of the embedding\n",
    "lstm_output, _ = lstm_layer(embed_output.unsqueeze(1))\n",
    "lstm_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655160d",
   "metadata": {},
   "source": [
    "## Classifiying the LSTM output\n",
    "\n",
    "We can now add a fully connected, [linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer to our NN to learn the correct part of speech (classification).\n",
    "\n",
    "**Creating a linear layer**. We create a linear layer by specifying the shape of the input into the layer and the number of neurons in the linear layer.\n",
    "\n",
    "**Linear layer input and output**. The input is expected to be (input_size, output_size) and the output will be the output of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75440441",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(hidden_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefce339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 1, 1]),\n",
       " tensor([[[-0.1629]],\n",
       " \n",
       "         [[-0.1743]],\n",
       " \n",
       "         [[-0.1828]],\n",
       " \n",
       "         [[-0.1887]],\n",
       " \n",
       "         [[-0.1928]],\n",
       " \n",
       "         [[-0.1954]],\n",
       " \n",
       "         [[-0.1970]],\n",
       " \n",
       "         [[-0.1979]],\n",
       " \n",
       "         [[-0.1986]],\n",
       " \n",
       "         [[-0.1990]],\n",
       " \n",
       "         [[-0.1992]],\n",
       " \n",
       "         [[-0.1993]],\n",
       " \n",
       "         [[-0.1994]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output = linear_layer(lstm_output)\n",
    "linear_output.shape, linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566eb1a2",
   "metadata": {},
   "source": [
    "# Training an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f687b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "valid_percent = 0.2  # Training/validation split\n",
    "\n",
    "embed_dim = 7  # Size of word embedding\n",
    "hidden_dim = 8  # Size of LSTM internal state\n",
    "num_layers = 5  # Number of LSTM layers\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27273ef",
   "metadata": {},
   "source": [
    "## Creating training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f53886c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 80)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(dataset)\n",
    "vocab_size = len(word_to_index)  # Number of unique input words\n",
    "\n",
    "# Shuffle the data so that we can split the dataset randomly\n",
    "shuffle(dataset)\n",
    "\n",
    "split_point = int(N * valid_percent)\n",
    "valid_dataset = dataset[:split_point]\n",
    "train_dataset = dataset[split_point:]\n",
    "\n",
    "len(valid_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69bd9c",
   "metadata": {},
   "source": [
    "## Creating the Parts of Speech LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3293433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_LSTM(torch.nn.Module):\n",
    "    \"\"\"Part of Speach LSTM model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X, _ = self.lstm(X.unsqueeze(1))\n",
    "        return self.linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee5bc3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c09deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset):\n",
    "    \"\"\"A helper function for computing accuracy on the given dataset.\"\"\"\n",
    "    total_words = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in dataset:\n",
    "            sentence_indices = convert_to_index_tensor(sentence, word_to_index)\n",
    "            tag_scores = model(sentence_indices).squeeze()\n",
    "            predictions = tag_scores.argmax(dim=1)\n",
    "            total_words += len(sentence)\n",
    "            total_correct += sum(t == tag_list[p] for t, p in zip(tags, predictions))\n",
    "\n",
    "    return total_correct / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c875a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = POS_LSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "mb = master_bar(range(num_epochs))\n",
    "\n",
    "# accuracy = compute_accuracy(valid_dataset)\n",
    "# print(f\"Validation accuracy before training : {accuracy * 100:.2f}%\")\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    # Shuffle the data for each epoch (stochastic gradient descent)\n",
    "    shuffle(train_dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for sentence, tags in progress_bar(train_dataset, parent=mb):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence = convert_to_index_tensor(sentence, word_to_index)\n",
    "#         tags = convert_to_index_tensor(tags, tag_to_index)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        break\n",
    "#         loss = criterion(tag_scores.squeeze(), tags)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# accuracy = compute_accuracy(valid_dataset)\n",
    "# print(f\"Validation accuracy after training : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e780e3",
   "metadata": {},
   "source": [
    "## Examining results\n",
    "\n",
    "Here we look at all words that are misclassified by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc42165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMis-predictions after training on entire dataset\")\n",
    "header = \"Word\".center(14) + \" | True Tag | Prediction\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in dataset:\n",
    "        sentence_indices = convert_to_index_tensor(sentence, word_to_index)\n",
    "        tag_scores = model(sentence_indices)\n",
    "        predictions = tag_scores.squeeze().argmax(dim=1)\n",
    "        for word, tag, pred in zip(sentence, tags, predictions):\n",
    "            if tag != tag_list[pred]:\n",
    "                print(f\"{word:>14} |     {tag}    |    {tag_list[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf48486",
   "metadata": {},
   "source": [
    "## Using the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"3 + 3\"\n",
    "\n",
    "# Convert sentence to lowercase words\n",
    "sentence = ps(new_sentence)\n",
    "\n",
    "# Check that each word is in our vocabulary\n",
    "for word in sentence:\n",
    "    assert word in word_to_index\n",
    "\n",
    "# Convert input to a tensor\n",
    "sentence = convert_to_index_tensor(sentence, word_to_index)\n",
    "\n",
    "# Compute prediction\n",
    "predictions = model(sentence)\n",
    "predictions = predictions.squeeze().argmax(dim=1)\n",
    "\n",
    "# Print results\n",
    "for word, tag in zip(ps(new_sentence), predictions):\n",
    "    print(word, \"=>\", tag_list[tag.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03126959",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "- compare with fully connected network\n",
    "- compare with CNN\n",
    "- compare with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d17307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bbd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
